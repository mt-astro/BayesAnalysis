X[1, ] <- xo
a[1] <- 2.5
b[1] <- mean(yo)
Y[1, ] = a[1]*X[1, ] + b[1]
lnsx[1] <- log(0.1)
lnsy[1] <- log(0.1)
lnpost[1] <- # starting value of posterior
posterior(xo=xo, yo=yo, x=X[1,], a=a[1], b=b[1],
lnsx=lnsx[1], lnsy=lnsy[1], nu=nu, log=TRUE)
accept_vals = runif(Ns)
Nacc = 0
for (i in 2:Ns){
c(xs, ys, ps, lnps) %<-% gen_prop(X[i-1, ], exp(lnsx[i-1]), Y[i-1, ], exp(lnsy[i-1]), c(a[i-1], b[i-1]), c(psda, psdb), c(lnsx[i-1], lnsy[i-1]), c(sdlnsx, sdlnsy))
c(as, bs) %<-% ps
c(lnsxs, lnsys) %<-% lnps
lnpost1 = posterior(xs, ys, x, as, bs, lnsxs, lnsys, log=TRUE)
if ( (i > 10) && (lnpost[i-1] == lnpost[i-5])) {
ratio = 1.
}
else {
ratio = exp(lnpost1 - lnpost[i-1])
}
print(lnpost1)
print(lnpost[i-1])
if (ratio > accept_vals[i]) {
X[i, ] = xs
Y[i, ] = xs*as + bs
a[i] = as
b[i] = bs
lnsx[i] = lnsxs
lnsy[i] = lnsys
lnpost[i] = lnpost1
Nacc = Nacc + 1
}
else {
X[i, ] = X[i-1, ]
Y[i, ] = Y[i-1, ]
a[i] = a[i-1]
b[i] = b[i-1]
lnsx[i] = lnsx[i-1]
lnsy[i] = lnsy[i-1]
lnpost[i] = lnpost[i-1]
}
}
## Discard first 500 samples as burn-in
burnin <- 500
a <- a[(burnin+1):Ns]
b <- b[(burnin+1):Ns]
X <- X[(burnin+1):Ns, ]
Y <- Y[(burnin+1):Ns, ]
lnsx <- lnsx[(burnin+1):Ns]
lnsy <- lnsy[(burnin+1):Ns]
lnpost <- lnpost[(burnin+1):Ns]
# estimate the MAP, the point at which the posterior is greatest
jmap <- which.max(lnpost)
amap <- a[jmap]
bmap <- b[jmap]
## Restore the x-data, for use in plotting below.
xo <- xoo
par(mfcol=c(1,3))
hist(a)
hist(b)
## Reduce the opacity of "black" for plotting
## Otherwise the scatterplot is nearly solid black
myblack <- adjustcolor("black", alpha.f=0.25)
plot(a, b, pch=".", col=myblack)
Nacc
a
plot(1:5000, lnpost)
rm(list=ls()) # clean up
library(knitr)
library(coda)
library(metRology, quietly=TRUE, warn.conflicts=FALSE)
library(zeallot)
gr <- (1+sqrt(5))/2 # golden ratio, for figures
opts_chunk$set(comment="  ",
#cache=TRUE,
#autodep=TRUE
eval.after="fig.cap",
collapse=TRUE,
dev="png",
fig.width=7.0,
out.width="95%",
fig.asp=0.9/gr,
fig.align="center"
)
par(mar=c(4, 4, 1, 1),
mgp=c(2.5, 1, 0),
bg="grey97")
par(mar=c(4, 4, 1, 1),
mgp=c(2.5, 1, 0),
bg="grey97")
#set.seed(seed=5, kind="Mersenne-Twister")
set.seed(seed=9, kind="Mersenne-Twister")
N <- 20     # number of data pairs
a <- 1.0    # slope
b <- 1.0    # intercept
if (c(TRUE, FALSE)[1]) { # hard test
x  <- runif(N, min=2.0, max=7.0)
x  <- sort(x) # x values
sx  <- 0.2    # SDx
sy  <- 0.2    # SDy
r  <- 0.2    # prevalence of y-outliers
sy1 <- 3.0    # SD for y-outliers
} else {  # easy test
x   <- seq(from=1, to=10, length.out=N)
sx  <- 0.1
sy  <- 0.1    # SDy
r  <- 0.1    # prevalence of y-outliers
sy1 <- 0.1
}
y <- a*x + b # y values
xo <- rnorm(N, mean=x, sd=sx) # observe x's
yo <- rnorm(N, mean=y, sd=sy) # observe y's
xo <- xo[order(xo)]
yo <- yo[order(xo)]
## make y-outliers
ii <- # indices of the yo to be overwritten
sample(1:N, size=round(r*N), replace=FALSE)
yo[ii] <- # outliers
rnorm(length(ii), mean=y[ii], sd=sy1)
## lm() fit
fit <- lm(yo ~ xo)
alm <- coef(fit)[2]; blm <- coef(fit)[1]
## plot data, true model and lm() fit
plot(xo, yo, bty="l", panel.first=grid(),
xlab="x", ylab="y", xaxs="i", yaxs="i",
xlim=range(xo)+c(-1,1), ylim=range(yo)+c(-1,1))
lines(x=c(0,11), y=a*c(0,11)+b, col="black")
abline(fit, col="red")
leg2 <- paste0("true model: a=", signif(a,2), ", ",
"b=", signif(  b,2) )
leg3 <- paste0("lm() fit: a=", signif(alm,2), ", ",
"b=", signif(blm,2) )
legend("bottomright", bty="o", box.col="gray50",
title=expression(italic(y==ax+b)),
inset=c(0.05, 0.05),
legend=c("data", leg2, leg3),
lty=c(NA, 1, 1), pch=c(1, NA, NA),
col=c("black", "black", "red"))
fig.cap <- paste0("**Figure 1.** Synthetic data for a linear regression
problem with y-outliers and errors in x-values. ",
"There are ", N, " x-y pairs in the dataset. ",
"The x-values have SD =", sx, " and the y-values
have SD =", sy, " except that ", length(ii),
" of the y-values are outliers with SD =", sy1,
". However, as data analysts, we cannot know which
points are outliers, or even whether outliers
exist in the data. All we know is that our x-meter is
reliable (if not perfectly accurate) but that our
y-meter has occasional glitches. Under these circumstances
the best we can do is use a model that is robust
to y-value outliers.")
#general functions used in the solution
#####
# failed solution, see next chunk instead
#####
gen_prop = function(x, dx, y, dy, p, dp, lnp, dlnp) {
xs = rnorm(length(x), mean=x, sd = dx)
ys = rnorm(length(y), mean=y, sd=dy)
ps = rnorm(length(p), mean=p, sd=dp)
lnps = rnorm(length(lnp), mean=lnp, sd=dlnp)
return (list( xs=xs, ys=ys, ps=ps , lnps = lnps))
}
evalPost = function(x0vec, xovec, yvec, pvec) {
#unpack param vec
a = pvec[1]
b = pvec[2]
sx = exp(pvec[3])
sy = exp(pvec[4])
#compute model yvals and diff
yi = a*xovec + b
ydiff = yi - yvec
#print(yvec)
#put it all together
term1 = dt.scaled(ydiff, df=2, mean=0., sd=sy, log=TRUE)
term2 = dnorm(xovec, mean=x0vec, sd=sx, log=TRUE)
return(exp(sum(term1+term2)))
}
#manual scaled t-dist
dts <- function(x, mean=0, logsd=0, sd=exp(logsd), df=2, log=FALSE) {
if(log) {
dt((x-mean)/sd, df=df, log=TRUE) - logsd
} else {
dt((x-mean)/sd, df=df, log=FALSE)/sd
}
}
posterior <- function(xo,yo,x,a,b,lnsx,lnsy,nu=2,log=FALSE){
## arguments
#  x[1:N], xo[1:N] true and observed x-values
#  yo[1:N] observed y-values
#  a, slope, and b, intercept (y=ax+b)
#  lnsx, lnsy logged scale factors for sampling distributions
#  nu, the normality
#  log, if(log=TRUE) return log(posterior)
N <- length(xo) # number of x-y pairs observed
logsum <- -lnsx - lnsy +
sum(dnorm(xo, mean=x, sd=exp(lnsx), log=TRUE) +
dts(yo, mean=a*x+b, logsd=lnsy, df=nu, log=TRUE))
# return posterior
if(log) logsum else exp(logsum)
}
#initial guesses
# known values given earlier in the doc, but we'll "guess" here
psdx = 0.1
psdy = 0.1
pa = 0.9
psda = 0.1
pb = 1.1
psdb = 0.1
#compute some generic stuff
sdlnsx = 0.25 #log(psdx)
sdlnsy = 0.25 #log(psdy)
xoc = xo - mean(xo)
Ns <- 5500 # number of samples, make it 1500 while debugging
nu <- 2    # normality of y-data
## preallocate storage
X <- matrix(double(Ns*N), nrow=Ns)
Y <- matrix(double(Ns*N), nrow=Ns)
a <- double(Ns)
b <- double(Ns)
lnsy <- double(Ns)
lnsx <- double(Ns)
lnpost <- double(Ns) # log posterior
## center x-values
xobar <- mean(xo)
xoo <- xo         # keep original x-data
xo  <- xo - xobar # centered x-data
## initial values
X[1, ] <- xo
a[1] <- 1.5
b[1] <- mean(yo)
Y[1, ] = a[1]*X[1, ] + b[1]
lnsx[1] <- log(0.1)
lnsy[1] <- log(0.1)
lnpost[1] <- # starting value of posterior
posterior(xo=xo, yo=yo, x=X[1,], a=a[1], b=b[1],
lnsx=lnsx[1], lnsy=lnsy[1], nu=nu, log=TRUE)
accept_vals = runif(Ns)
Nacc = 0
for (i in 2:Ns){
c(xs, ys, ps, lnps) %<-% gen_prop(X[i-1, ], exp(lnsx[i-1]), Y[i-1, ], exp(lnsy[i-1]), c(a[i-1], b[i-1]), c(psda, psdb), c(lnsx[i-1], lnsy[i-1]), c(sdlnsx, sdlnsy))
c(as, bs) %<-% ps
c(lnsxs, lnsys) %<-% lnps
lnpost0 = posterior(X[i-1, ], yo, x, a[i-1], b[i-1], lnsx[i-1], lnsy[i-1], log=TRUE)
lnpost1 = posterior(xs, yo, x, as, bs, lnsxs, lnsys, log=TRUE)
ratio = exp(lnpost1 - lnpost0)
if (ratio > accept_vals[i]) {
X[i, ] = xs
Y[i, ] = xs*as + bs
a[i] = as
b[i] = bs
lnsx[i] = lnsxs
lnsy[i] = lnsys
lnpost[i] = lnpost1
Nacc = Nacc + 1
}
else {
X[i, ] = X[i-1, ]
Y[i, ] = Y[i-1, ]
a[i] = a[i-1]
b[i] = b[i-1]
lnsx[i] = lnsx[i-1]
lnsy[i] = lnsy[i-1]
lnpost[i] = lnpost0
}
}
## Discard first 500 samples as burn-in
burnin <- 500
a <- a[(burnin+1):Ns]
b <- b[(burnin+1):Ns]
X <- X[(burnin+1):Ns, ]
Y <- Y[(burnin+1):Ns, ]
lnsx <- lnsx[(burnin+1):Ns]
lnsy <- lnsy[(burnin+1):Ns]
lnpost <- lnpost[(burnin+1):Ns]
# estimate the MAP, the point at which the posterior is greatest
jmap <- which.max(lnpost)
amap <- a[jmap]
bmap <- b[jmap]
## Restore the x-data, for use in plotting below.
xo <- xoo
par(mfcol=c(1,3))
hist(a)
hist(b)
## Reduce the opacity of "black" for plotting
## Otherwise the scatterplot is nearly solid black
myblack <- adjustcolor("black", alpha.f=0.25)
plot(a, b, pch=".", col=myblack)
Nacc
rm(list=ls()) # clean up
library(knitr)
library(coda)
library(metRology, quietly=TRUE, warn.conflicts=FALSE)
library(zeallot)
gr <- (1+sqrt(5))/2 # golden ratio, for figures
opts_chunk$set(comment="  ",
#cache=TRUE,
#autodep=TRUE
eval.after="fig.cap",
collapse=TRUE,
dev="png",
fig.width=7.0,
out.width="95%",
fig.asp=0.9/gr,
fig.align="center"
)
par(mar=c(4, 4, 1, 1),
mgp=c(2.5, 1, 0),
bg="grey97")
par(mar=c(4, 4, 1, 1),
mgp=c(2.5, 1, 0),
bg="grey97")
#set.seed(seed=5, kind="Mersenne-Twister")
set.seed(seed=9, kind="Mersenne-Twister")
N <- 20     # number of data pairs
a <- 1.0    # slope
b <- 1.0    # intercept
if (c(TRUE, FALSE)[1]) { # hard test
x  <- runif(N, min=2.0, max=7.0)
x  <- sort(x) # x values
sx  <- 0.2    # SDx
sy  <- 0.2    # SDy
r  <- 0.2    # prevalence of y-outliers
sy1 <- 3.0    # SD for y-outliers
} else {  # easy test
x   <- seq(from=1, to=10, length.out=N)
sx  <- 0.1
sy  <- 0.1    # SDy
r  <- 0.1    # prevalence of y-outliers
sy1 <- 0.1
}
y <- a*x + b # y values
xo <- rnorm(N, mean=x, sd=sx) # observe x's
yo <- rnorm(N, mean=y, sd=sy) # observe y's
xo <- xo[order(xo)]
yo <- yo[order(xo)]
## make y-outliers
ii <- # indices of the yo to be overwritten
sample(1:N, size=round(r*N), replace=FALSE)
yo[ii] <- # outliers
rnorm(length(ii), mean=y[ii], sd=sy1)
## lm() fit
fit <- lm(yo ~ xo)
alm <- coef(fit)[2]; blm <- coef(fit)[1]
## plot data, true model and lm() fit
plot(xo, yo, bty="l", panel.first=grid(),
xlab="x", ylab="y", xaxs="i", yaxs="i",
xlim=range(xo)+c(-1,1), ylim=range(yo)+c(-1,1))
lines(x=c(0,11), y=a*c(0,11)+b, col="black")
abline(fit, col="red")
leg2 <- paste0("true model: a=", signif(a,2), ", ",
"b=", signif(  b,2) )
leg3 <- paste0("lm() fit: a=", signif(alm,2), ", ",
"b=", signif(blm,2) )
legend("bottomright", bty="o", box.col="gray50",
title=expression(italic(y==ax+b)),
inset=c(0.05, 0.05),
legend=c("data", leg2, leg3),
lty=c(NA, 1, 1), pch=c(1, NA, NA),
col=c("black", "black", "red"))
fig.cap <- paste0("**Figure 1.** Synthetic data for a linear regression
problem with y-outliers and errors in x-values. ",
"There are ", N, " x-y pairs in the dataset. ",
"The x-values have SD =", sx, " and the y-values
have SD =", sy, " except that ", length(ii),
" of the y-values are outliers with SD =", sy1,
". However, as data analysts, we cannot know which
points are outliers, or even whether outliers
exist in the data. All we know is that our x-meter is
reliable (if not perfectly accurate) but that our
y-meter has occasional glitches. Under these circumstances
the best we can do is use a model that is robust
to y-value outliers.")
#general functions used in the solution
#####
# failed solution, see next chunk instead
#####
gen_prop = function(x, dx, y, dy, p, dp, lnp, dlnp) {
xs = rnorm(length(x), mean=x, sd = dx)
ys = rnorm(length(y), mean=y, sd=dy)
ps = rnorm(length(p), mean=p, sd=dp)
lnps = rnorm(length(lnp), mean=lnp, sd=dlnp)
return (list( xs=xs, ys=ys, ps=ps , lnps = lnps))
}
evalPost = function(x0vec, xovec, yvec, pvec) {
#unpack param vec
a = pvec[1]
b = pvec[2]
sx = exp(pvec[3])
sy = exp(pvec[4])
#compute model yvals and diff
yi = a*xovec + b
ydiff = yi - yvec
#print(yvec)
#put it all together
term1 = dt.scaled(ydiff, df=2, mean=0., sd=sy, log=TRUE)
term2 = dnorm(xovec, mean=x0vec, sd=sx, log=TRUE)
return(exp(sum(term1+term2)))
}
#manual scaled t-dist
dts <- function(x, mean=0, logsd=0, sd=exp(logsd), df=2, log=FALSE) {
if(log) {
dt((x-mean)/sd, df=df, log=TRUE) - logsd
} else {
dt((x-mean)/sd, df=df, log=FALSE)/sd
}
}
posterior <- function(xo,yo,x,a,b,lnsx,lnsy,nu=2,log=FALSE){
## arguments
#  x[1:N], xo[1:N] true and observed x-values
#  yo[1:N] observed y-values
#  a, slope, and b, intercept (y=ax+b)
#  lnsx, lnsy logged scale factors for sampling distributions
#  nu, the normality
#  log, if(log=TRUE) return log(posterior)
N <- length(xo) # number of x-y pairs observed
logsum <- -lnsx - lnsy +
sum(dnorm(xo, mean=x, sd=exp(lnsx), log=TRUE) +
dts(yo, mean=a*x+b, logsd=lnsy, df=nu, log=TRUE))
# return posterior
if(log) logsum else exp(logsum)
}
#initial guesses
# known values given earlier in the doc, but we'll "guess" here
psdx = 0.1
psdy = 0.1
pa = 0.9
psda = 0.1
pb = 1.1
psdb = 0.1
#compute some generic stuff
sdlnsx = 0.025 #log(psdx)
sdlnsy = 0.025 #log(psdy)
xoc = xo - mean(xo)
Ns <- 5500 # number of samples, make it 1500 while debugging
nu <- 2    # normality of y-data
## preallocate storage
X <- matrix(double(Ns*N), nrow=Ns)
Y <- matrix(double(Ns*N), nrow=Ns)
a <- double(Ns)
b <- double(Ns)
lnsy <- double(Ns)
lnsx <- double(Ns)
lnpost <- double(Ns) # log posterior
## center x-values
xobar <- mean(xo)
xoo <- xo         # keep original x-data
xo  <- xo - xobar # centered x-data
## initial values
X[1, ] <- xo
a[1] <- 1.5
b[1] <- mean(yo)
Y[1, ] = a[1]*X[1, ] + b[1]
lnsx[1] <- log(0.1)
lnsy[1] <- log(0.1)
lnpost[1] <- # starting value of posterior
posterior(xo=xo, yo=yo, x=X[1,], a=a[1], b=b[1],
lnsx=lnsx[1], lnsy=lnsy[1], nu=nu, log=TRUE)
accept_vals = runif(Ns)
Nacc = 0
for (i in 2:Ns){
c(xs, ys, ps, lnps) %<-% gen_prop(X[i-1, ], exp(lnsx[i-1]), Y[i-1, ], exp(lnsy[i-1]), c(a[i-1], b[i-1]), c(psda, psdb), c(lnsx[i-1], lnsy[i-1]), c(sdlnsx, sdlnsy))
c(as, bs) %<-% ps
c(lnsxs, lnsys) %<-% lnps
lnpost0 = posterior(X[i-1, ], yo, x, a[i-1], b[i-1], lnsx[i-1], lnsy[i-1], log=TRUE)
lnpost1 = posterior(xs, yo, x, as, bs, lnsxs, lnsys, log=TRUE)
ratio = exp(lnpost1 - lnpost0)
if (ratio > accept_vals[i]) {
X[i, ] = xs
Y[i, ] = xs*as + bs
a[i] = as
b[i] = bs
lnsx[i] = lnsxs
lnsy[i] = lnsys
lnpost[i] = lnpost1
Nacc = Nacc + 1
}
else {
X[i, ] = X[i-1, ]
Y[i, ] = Y[i-1, ]
a[i] = a[i-1]
b[i] = b[i-1]
lnsx[i] = lnsx[i-1]
lnsy[i] = lnsy[i-1]
lnpost[i] = lnpost0
}
}
## Discard first 500 samples as burn-in
burnin <- 500
a <- a[(burnin+1):Ns]
b <- b[(burnin+1):Ns]
X <- X[(burnin+1):Ns, ]
Y <- Y[(burnin+1):Ns, ]
lnsx <- lnsx[(burnin+1):Ns]
lnsy <- lnsy[(burnin+1):Ns]
lnpost <- lnpost[(burnin+1):Ns]
# estimate the MAP, the point at which the posterior is greatest
jmap <- which.max(lnpost)
amap <- a[jmap]
bmap <- b[jmap]
## Restore the x-data, for use in plotting below.
xo <- xoo
par(mfcol=c(1,3))
hist(a)
hist(b)
## Reduce the opacity of "black" for plotting
## Otherwise the scatterplot is nearly solid black
myblack <- adjustcolor("black", alpha.f=0.25)
plot(a, b, pch=".", col=myblack)
Nacc
Nacc
