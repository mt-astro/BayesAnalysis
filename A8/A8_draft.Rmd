---
title: "A8_draft"
author: "Michael Tucker"
date: "2020-03-20"
output: 
  html_document: 
    theme: readable #paper #cosmo #journal #cerulean
    toc: true
    smooth_scroll: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    code_folding: hide
---  

<style type="text/css">  
/* Note: CSS uses C-style commenting. */
h1.title{font-size:22px; text-align:center;}
h4.author{font-size:16px; text-align:center;}
h4.date{font-size:16px; text-align:center;}
body{ /* Normal  */ font-size: 13px}
td {  /* Table   */ font-size: 12px}
h1 { /* Header 1 */ font-size: 16px}
h2 { /* Header 2 */ font-size: 14px}
h3 { /* Header 3 */ font-size: 12px}
.math{ font-size: 10pt;}
.hi{ /* hanging indents */ 
    padding-left:22px; 
    text-indent:-22px;
}
blockquote {  
    padding: 10px 20px;
    margin: 0 0 20px;
    font-size: 12px;
    border-left: 5px solid #eee;
}
code.r{ /* code */ 
       font-size: 12px;
}
pre{/*preformatted text*/ 
    font-size: 12px;
}
p.caption {/* figure captions */ 
    font-size: 1.0em;
    font-style: italic; 
} 
</style>

```{r setup, echo=FALSE}
rm(list=ls()) # clean up
library(knitr)
library(coda)
library(metRology, quietly=TRUE, warn.conflicts=FALSE)
gr <- (1+sqrt(5))/2 # golden ratio, for figures
opts_chunk$set(comment="  ",
               #cache=TRUE,
               #autodep=TRUE
               eval.after="fig.cap",
               collapse=TRUE, 
               dev="png",
               fig.width=7.0,
               out.width="95%",
               fig.asp=0.9/gr,
               fig.align="center"
               )
```  

# Introduction {-} 

This assignment treats _robust_ linear regression with _errors in variables_, a problem that is common in science and does not involve coins. Robustness and errors-in-variables are difficult for conventional methods, but easy for Bayesian methods. **Your assignment is** to code **either** Metropolis **or** the Gibbs sampler, then analyze the data with your code. 

This assignment is relevant to Chao's research, in which pressure, density and seismic soundspeeds are measured in diamond anvil cells and then extrapolated to great depth in the earth using the second order [Birch-Murnaghan equation of state](https://en.wikipedia.org/wiki/Birch%E2%80%93Murnaghan_equation_of_state). The outstanding question is whether such extrapolations agree with the PREM model for soundspeeds and density.  

It is also relevant to Andrew's research, in which estimates of seamount age and location in various island chains are used to estimate the absolute motion of the [Pacific plate](https://en.wikipedia.org/wiki/Plate_tectonics).    

In the following I often mix code-type notation with math notation, in order to remind you of distribution names and suggest names for variables in your codes. Beware that on some screens it is easy to mistake the tilde `~` for a minus sign. The tilde is how mathematicians, as well as the MCMC languages [BUGS](https://en.wikipedia.org/wiki/OpenBUGS), [JAGS](https://en.wikipedia.org/wiki/Just_another_Gibbs_sampler) and [Stan](https://en.wikipedia.org/wiki/Stan_(software)), indicate that the variable on the left is a sample from the distribution on the right.    

**We have data** `xo[i]`, `yo[i]`, `i=1:N`. The lower case "o" in these names is a reminder that these are _observed_ values, not the true values, `x[i], y[i]`, which are unknown because our measurement systems are imperfect.   

Our **process model** is the usual `y[i] = a*x[i] + b`. Notice that it involves the unknown true values of `x`, not the observed values `xo`. The slope `a` and intercept `b` are the only two parameters of our process model. It would be easy to have a more complicated process model with more parameters, but the process model is not our focus here. Our complete MCMC model includes the unknown x-values as variables/parameters.  

Our **x-observations** come from a device that we believe has a normal distribution of errors, so our observation model for the x-values is `xo[i] ~ dnorm(mean=x[i], sd=sx)`. In other words the observation `xo[i]` is assumed to be a draw from a normal distribution with mean `x[i]` and standard deviation `sx`. In statistics, the notation $s_x$ would be reserved for sample SD, and population or distribution SD would be denoted by $\sigma_x$. I am too lazy to type `sigma_x` everywhere, so I using `sx`.   

We suspect that some of our **y-observations** are _outliers_, hence our observation model for the y-values is a Student t-distribution `yo[i] ~ dt(mean=y[i], df=nu, sd=sy)` in which `nu` is the _normality_, more often referred to as the _degrees of freedom_. We use the t-distribution instead of the normal because it is [fat-tailed](https://en.wikipedia.org/wiki/Fat-tailed_distribution) compared to the normal and thus more tolerant of outliers. The [double-exponential (Laplace)](https://en.wikipedia.org/wiki/Laplace_distribution) is another JAGS-supplied distribution with fat tails that you may find useful someday.  

Kruschke (Section 16.2, page 458) gives a nice explanation of the t-distribution, noting that `nu` cannot be less than 1 and that the t-distribution becomes the normal distribution in the limit as `nu`$\rightarrow \infty$. Please read the entry for the Student t-distribution on page 48 of the [JAGS 4.3.0 User Manual](https://sourceforge.net/projects/mcmc-jags/files/Manuals/4.x/), noting that _k_ is used there to denote the normality parameter.  

# JAGS    
We are going to write our own MCMC code for this problem, but writing a JAGS model first may help to clarify our thinking, as well as making us appreciate the simplicity of JAGS. Some things to note about the JAGS language: (1) JAGS uses C-style commenting, in which comments have the form `/* comment */`. (2) JAGS is a [declarative language](https://en.wikipedia.org/wiki/Declarative_programming) so the order of the statements in a JAGS model does not matter. (Statement order _does_ matter in Stan.) (3) JAGS parameterizes the normal and t-distributions using precision $\tau=1/\sigma^2$ instead of standard deviation. Here is the model in exactly the form we would give to JAGS:  

```
model {
  for (i in 1:N) { /* observe x's and y's */
    xo[i] ~ dnorm(x[i], 1/(sx*sx))
    yo[i] ~ dt(y[i], 1/(sy*sy), nu)
  }
  
  for (i in 1:N) { /* deterministic process model */
    y[i] <- a*x[i] + b
  }
  
  for (i in 1:N) { /* priors for x's */
    x[i] ~ dunif(-10, 20)
  }
  
  /* more priors */
  a    ~ dunif(-10, 10)
  b    ~ dunif(-10, 10)
  lnsx ~ dunif(-10, 10)
  lnsy ~ dunif(-10, 10)
  
  /* more deterministic variables */
  sx  <- exp(lnsx)
  sy  <- exp(lnsy)
}
```  

For clarity I wrote the JAGS model using three loops instead of one. This does not cause a loss of efficiency because JAGS doesn't _execute_ the statements in the model; rather it uses them to build a [Bayesian network](https://en.wikipedia.org/wiki/Bayesian_network), sometimes known as a _probabilistic directed acyclic graph_ (DAG). Our textbook has many DAG diagrams: for example, Figure 9.7 on page 236 and Figure 17.2 on page 480. A DAG diagram is always helpful, but for non-hierarchical problems like this one I find that if I carefully scrutinize my JAGS model, I do not need to sketch the DAG for it first.   

Notice in the JAGS model that I used uniform priors for the x's, the slope, and the intercept. If I had prior information that the slope was positive I would have used the reciprocal prior. I do _not_ specify priors for the unknown y's because they are determined by the x's, slope and intercept. By using uniform priors for $\ln(s_x)$ and $\ln(s_y)$ I effectively get exact reciprocal priors for $s_x$ and $s_y$. Kruschke would have used `dgamma(0.001, 0.001)` as the prior for $s_x$ and $s_y$, and the interval-scale variables $\ln\sigma_x$ and $\ln\sigma_y$ would not be present in his analysis.  

Notice also in the JAGS model that the normality $\nu$ is a fixed parameter. In our analysis we will fix it at 2 or 3. The normality parameter is difficult to resolve unless one has a lot of data because it (jargon alert) _trades off_ with $\sigma_y$. Making $\nu$ smaller gives the t-distribution longer tails, but making $\sigma_y$ larger also gives it longer tails.   

Our textbook does robust linear regression in Section 17.2 (page 479), but it assumes known values of $x$, and it treats the normality $\nu$ and precision $1/\sigma_y^2$ as parameters.  

# Synthetic data  
Before trying any new code on real data one should always try it first on synthetic data, or simulated data as it is called in the social and life sciences. In the following chunk I synthesize a data set with outliers in the y-observations.  

```{r synthetics, fig.cap=fig.cap}
par(mar=c(4, 4, 1, 1),
    mgp=c(2.5, 1, 0),
    bg="grey97")
set.seed(2)
N <- 20     # number of data pairs
a <- 1.0    # slope
b <- 1.0    # intercept 
sx <- 0.5   # SDx
sy  <- 1.0  # SDy
sy1 <- 5.0  # SD for y-outliers
r  <-  0.2  # prevalence of y-outliers

x <- runif(N, min=2.0, max=7.0)
x <- sort(x) # x values
y <- a*x + b # y values
xo <- rnorm(N, mean=x, sd=sx) # observe x's
yo <- rnorm(N, mean=y, sd=sy) # observe y's
xo <- xo[order(xo)]
yo <- yo[order(xo)]

## make y-outliers
ii <- # indices of the yo to be overwritten
  sample(1:N, size=round(r*N), replace=FALSE)
yo[ii] <- # outliers
  rnorm(length(ii), mean=x[ii], sd=sy1)

## plot data, true model and lm() fit
plot(xo, yo, bty="l", panel.first=grid(),
     xlab="x", ylab="y", xaxs="i", yaxs="i",
     xlim=c(0, 7.5), ylim=c(-6, 12))
lines(x=c(0, 10), a*c(0,10)+b, col="black")
abline(lm(yo ~ xo), col="red")
legend("topleft", inset=0.05, bty="o", box.col="gray50",
       legend=c("observations", "true model", "lm() fit"),
       lty=c(NA, 1, 1), pch=c(1, NA, NA),
       col=c("black", "black", "red"))
fig.cap <- paste0("**Figure 1.** Synthetic data for a linear regression 
                  problem with y-outliers and errors in x-values. ", 
                  "There are ", N, " x-y pairs in the dataset. ",
                  "The x-values have SD =", sx, " and the y-values
                  have SD =", sy, " except that ", length(ii), 
                  " of the y-values are outliers with SD =", sy1,
                  ". However, as data analysts, we cannot know which
                  points are outliers, or even whether outliers
                  exist in the data. All we know is that our x-meter is 
                  reliable (if not perfectly accurate) but that our 
                  y-meter has occasional glitches. Under these circumstances 
                  the best we can do is use a model that is robust 
                  to y-value outliers.")
```

# Centering data  
Most regression software _centers_ and _scales_ regression data before analyzing it, and so should yours. Why is that? Notice in Figure 1 that if the slope increases a bit the intercept will decrease a bit. The parameters $a$ and $b$ will thus be (anti) correlated and the effective sample size of your MCMC samples will be reduced. In the problem of this assignment it is sufficient to center the x-values: `xoc <- xo - mean(xo)`. To see the posterior effect of this consider our process model in the form  

$$\begin{align}
y 
&= a\cdot x + b \\
&= a\cdot[x_c + {\rm mean}(x^{(o)})] + b \\
&= a\cdot x_c + [b+a\cdot {\rm mean}(x^{(o)})]\\
&= a\cdot x_c + b_c
\end{align}$$  

When we have finished sampling, we will have samples `a[j]` and `bc[j]` for `j=1,2,...,Ns` and the true intercept sample `b[j]` is given by `b[j] = bc[j] - a[j]*mean(xo)`.  

# Mathematical model  

(Skip this math if you like, but look at the result, equation (6*).)

As we intend to write our own MCMC code for this problem we now put JAGS aside and lay out the mathematical model in the canonical form $p(\beta\vert D) p(D) = p(D\vert \beta) p(\beta)$. In order to avoid mistakes we begin as usual with a tautology for the joint distribution of parameters and data:  

$$\begin{align}
\rm{LHS} &= \rm{RHS} \\
p(a,b,x,y,\sigma_x, \sigma_y, \nu, \ln\sigma_x, \ln\sigma_y, x_o, y_o) 
&= 
p(a,b,x,y,\sigma_x, \sigma_y, \nu, \ln\sigma_x, \ln\sigma_y, x_o, y_o),
\end{align} \tag{1}$$  

in which $x, x_o, y$ and $y_o$ are `r N`-element vectors.  

We rework the RHS by repeated use of the product rule, obtaining  

$$\begin{align}
{\rm RHS} &=
p(y_o\vert y, \sigma_y, \nu)\,p(y \vert x, a, b)\,p(\sigma_y \vert \ln\sigma_y)
\ p(\ln \sigma_y)\\
&\times
p(x_o\vert x, \sigma_x)\ p(\sigma_x \vert \ln\sigma_x) \tag{2a}
\ p(\ln \sigma_x)\\
&\times
 p(x)\, p(a)\, p(b) \\ \\
&={\rm dt}(y_o\vert y, \sigma_y, \nu)\, \delta(y-ax-b)\, 
\delta(\sigma_y-e^{\ln \sigma_y})
\, {\rm dunif}(\ln \sigma_y)\\
&\times {\rm dnorm}(x_0\vert x, \sigma_x)\, \delta(\sigma_x-e^{\ln \sigma_x})
\, {\rm dunif}(\ln \sigma_x) \tag{2b}\\
&\times {\rm dunif}(x)\, {\rm dunif}(a)\, 
{\rm dunif}(b).
\end{align}$$

Notice that in equation (2b) we made use of the [Dirac delta function](https://en.wikipedia.org/wiki/Dirac_delta_function) $\delta()$ to handle the deterministic nodes in our JAGS model. Paul Dirac, the physicist who was a close second to Einstein for brilliance, casually introduced his delta function in a quantum mechanics textbook, and mathematicians had their drawers in knots for a decade until they invented the theory of [generalized functions](https://en.wikipedia.org/wiki/Generalized_function) to deal with it. All we need to know is that $\delta(x-a)$ is a PDF with all of its probability mass concentrated at the single point $x=a$, and thus for any well-behaved function $f(x)$ the expectation of $f$ under the delta PDF is $E(f)=\int\! \delta(x-a)\,f(x)\,dx=f(a)$.  

Integrating the RHS over the deterministic variables gives  

$$\begin{align}
\small\int\! dy\ d\sigma_x \, d\sigma_y\ ({\rm RHS})
&={\rm dt}(y_o\vert a+bx, e^{\ln \sigma_y}, \nu) 
\, {\rm dunif}(\ln \sigma_y)\\
&\times {\rm dnorm}(x_0\vert x, e^{\ln \sigma_x})
\, {\rm dunif}(\ln \sigma_x) \tag{3}\\
&\times {\rm dunif}(x)\, {\rm dunif}(a)\, {\rm dunif}(b).
\end{align}$$  

Next we turn our attention to the left hand side of equation (1) and use the product rule to write it as a posterior distribution for the parameters times a prior predictive distribution for the data. After several applications of the product rule, and noting the independence of some variables from other variables we obtain  

$$\begin{align}
\rm{LHS} &=
p(y\vert x,a,b)\, p(\sigma_x\vert\ln\sigma_x)\, p(\sigma_y\vert\ln\sigma_y)\\
&\times p(x, a, b, \ln\sigma_x, \ln\sigma_y \vert x_o, y_o, \nu) \tag{4a} \\ 
&\times p(x_0, y_0\vert\nu)\\
\\
&= \delta(y - ax - b)\, \delta(\sigma_x - e^{\ln\sigma_x})\,
\delta(\sigma_y - e^{\ln\sigma_y}) \\
&\times p(x, a, b, \ln\sigma_x, \ln\sigma_y \vert x_o, y_o, \nu)\tag{4b} \\
&\times p(x_0, y_0\vert\nu) 
\end{align}$$  

As we did above for the RHS, we integrate the LHS over the deterministic variables $y$, $\sigma_x$ and $\sigma_y$ and obtain  

$$\begin{align}
\small\int\! dy\ d\sigma_x \, d\sigma_y\ ({\rm LHS})
&= p(x, a, b, \ln\sigma_x, \ln\sigma_y \vert x_o, y_o, \nu)\, 
p(x_0, y_0\vert\nu). \tag{5}
\end{align}$$  

Notice that each of the delta functions in the first line of (4b) integrates to 1, and there is no replacement in the second line of (4b) because neither $y$, nor $\sigma_x$, nor $\sigma_y$ is present there.

The first factor in (5) is the posterior, and the second factor, $p(x_o,y_o\vert\nu)$, is the prior predictive. Setting the integrated RHS equal to the integrated LHS and dividing by the prior predictive gives our final result: 

$$\begin{align}
p(x, a, b, \ln\sigma_x, \ln\sigma_y \vert x_o, y_o, \nu)
&\propto 
{\rm dt}(y_o\vert a+bx, e^{\ln \sigma_y}, \nu) 
\, {\rm dunif}(\ln \sigma_y)\\
&\times {\rm dnorm}(x_0\vert x, e^{\ln \sigma_x})
\, {\rm dunif}(\ln \sigma_x) \tag{6*}\\
&\times {\rm dunif}(x)\, {\rm dunif}(a)\, {\rm dunif}(b).
\end{align}$$  

The derivation above was somewhat tedious because we used the variables $\ln\sigma_x$ and $\ln\sigma_y$ instead of $\sigma_x$ and $\sigma_y$, respectively. If this exercise were in our textbook the variables $\sigma_x$ and $\sigma_y$ would have been used and the final result would have been  

$$\begin{align}
p(x, a, b, \sigma_x, \sigma_y \vert\, x_o, y_o, \nu)
&\propto 
{\rm dt}(y_o\vert\, a+bx, \sigma_y, \nu) 
\, {\rm dgamma}(\sigma_y\vert\, 0.001, 0.001)\\
&\times {\rm dnorm}(x_0\vert\, x, \sigma_x)
\, {\rm dgamma}(\sigma_x\vert\, 0.001,0.001) \tag{7}\\
&\times {\rm dunif}(x)\, {\rm dunif}(a)\, {\rm dunif}(b).
\end{align}$$  

# Metropolis  

To generate samples from equation (6) with Metropolis I suggest you use a multivariate normal as the proposal distribution and that you do not use limits on your uniform priors. Then all the instances of `dunif()` in (6) are replaced by 1, and the posterior to be evaluated becomes  

$$
{\rm post}(x, a, b, \ln\sigma_x, \ln\sigma_y \vert x_o, y_o, \nu) =\\
\prod_{i=1}^N {\rm dt}(y^{(o)}_i \vert\, ax_i + b, e^{\ln\sigma_y}, \nu)
\cdot {\rm dnorm}(x^{(o)}_i \vert\, x_i, e^{\ln\sigma_x}) \tag{8}
$$  

A multivariate normal with diagonal covariance matrix is just the product of univariate normals. Accordingly, for your proposal distribution I suggest you use a univariate normal for each element of the $N + 4$ length vector of variables 
$\beta=[x_1, x_2,\ldots,x_N, y_1, y_2,\ldots,y_N, a, b, \ln\sigma_x, \ln\sigma_y]$. Tune the algorithm by adjusting the standard deviations of the proposal normals.

To make this more concrete, suppose we are at position  
```
beta[j] = (x[1,j], x[2,j],..., x[N,j], y[1,j], y[2,j],..., 
           y[N,j], a[j], b[j], lnsx[j], lnsy[j])
```  

in our $N+4$ dimensional parameter space.   

The $x_1$ element of the proposal vector is a draw  
`   xp[1] ~ dnorm(mean=x[1,j], sd=sdpx)`;  
the $x_2$ element of the proposal vector is a draw  
`xp[2] ~ dnorm(mean=x[2,j], sd=sdpx)`,...;  
the $x_N$ element of the proposal vector is a draw  
`xp[N] ~ dnorm(mean=x[N,j], sd=sdpx)`;  
the $y_1$ element of the proposal vector is a draw  
`yp[1] ~ dnorm(mean=y[1,j], sd=sdpy)`;  
the $y_2$ element of the proposal vector is a draw  
`yp[2] ~ dnorm(mean=y[2,j], sd=sdpy)`,...;  
the $y_N$ element of the proposal vector is a draw  
`yp[N] ~ dnorm(mean=y[N,j], sd=sdpy)`;  
the $a$ element of the proposal vector is a draw  
`ap ~ dnorm(mean=a[j], sd=sdpa)`;  
the $b$ element of the proposal vector is a draw  
`bp ~ dnorm(mean=b[j], sd=sdpb)`;  
the $\ln\sigma_x$ element of the proposal vector is a draw  
`lnsxp ~ dnorm(mean=lnsx[j], sd=sdplnsx)`; and  
the $\ln\sigma_y$ element of the proposal vector is a draw  
`lnsyp ~ dnorm(mean=lnsy[j], sd=sdplnsy)`.   

You can get all those draws in one call by putting the SDs into a vector. Thus  
```  
SD <- c(rep(sdpx, N), rep(sdpy, N), sdpa, sdpb, sdplnsx, sdplnsy)
beta_prop <-rnorm(N+4, mean=beta[j], sd=SD) 
```

One more thing: Notice that equation (8) is the product of many small quantities, hence numerical underflows are likely to occur. To prevent them, calculate the log-posterior first, then exponentiate it. In other words calculate your posterior as  

$$
\exp\sum_{i=1}^N \big[ \ln{\rm dt}(y^{(o)}_i \vert\, ax_i + b, e^{\ln\sigma_y}, \nu)
+ \ln{\rm dnorm}(x^{(o)}_i \vert\, x_i, e^{\ln\sigma_x})\big]. \tag{9}
$$  
The normal distribution in R is `dnorm(x, mean=0, sd=1, log=FALSE)` so set the log argument to `TRUE` in your call. The `dt.scaled()` function in the `metRology` package is `dt.scaled(x, df=, mean=0, sd=1, log=FALSE)`, so set `log=TRUE` in that call too.  


#Attempted Solution


```{r sol-func}

#general functions used in the solution

gen_prop = function(meanvec, sdvec) {
  n = length(meanvec)
  stopifnot( ( length(meanvec) == length(sdvec) ))
  sdvec[n-1] = exp(sdvec[n-1])
  sdvec[n] = exp(sdvec[n])
  return ( rnorm(n, meanvec, sdvec) )
}

evalPost = function(x0vec, xovec, yvec, pvec) {
  #unpack param vec
  a = pvec[1]
  b = pvec[2]
  sx = exp(pvec[3])
  sy = exp(pvec[4])

  #compute model yvals and diff   
  yi = a*xovec + b
  ydiff = yi - yvec
  #print(yvec)

  #put it all together
  term1 = dt.scaled(ydiff, df=2, mean=0., sd=sy, log=TRUE)
  term2 = dnorm(xovec, mean=x0vec, sd=sx, log=TRUE)
  return(exp(sum(term1+term2)))
}


```

```{r sol-setup}

#initial guesses
# known values given earlier in the doc, but we'll "guess" here
psx = 0.5
psdx = 1.
psy = 1.
psdy = 1.
pa = 0.99
psda = 0.8
pb = 1.01
psdb = 0.8

#compute some generic stuff
lnpsx = log(psx)
lnpsdx = log(psdx)
lnpsy = log(psy)
lnpsdy = log(psdy)
xoc = xo - mean(xo)

beta0 = c(xoc, yo, pa, pb, lnpsx, lnpsy)
SDs = c(rep(psdx, N), rep(psdy, N), psda, psdb, lnpsdx, lnpsdy)

```

```{r sol-run}

Niter=10000
np = length(beta0) - 2*N
p1 = length(beta0)
p0 = p1 - np

accept_vals = runif(Niter)

#vecs for output
a_out = vector(mode='double', length=Niter)
b_out = vector(mode='double', length=Niter)
lnsx_out = vector(mode='double', length=Niter)
lnsy_out = vector(mode='double', length=Niter)
post_out = vector(mode='double', length=Niter)
for (i in 1:Niter) {
  #get new beta
  beta1 = gen_prop(beta0, SDs)

  #eval posteriors  
  post0= evalPost(xoc, beta0[1:N], beta0[(N+1):(2*N)], beta0[p0:p1])
  post1 = evalPost(xoc, beta1[1:N], beta1[(N+1):(2*N)], beta1[p0:p1])

  #compute ratio and determine acceptance
  ratio = post1 / post0
  if (ratio > accept_vals[i]) {
    beta0 = beta1
    post = post1
  }
  else post=post0
  #store variables
  a_out[i] = beta0[p0]
  b_out[i] = beta0[p0+1]
  lnsx_out[i] = beta0[p0+2]
  lnsy_out[i] = beta0[p0+3]
  post_out[i] = post
  
}

#shift intercepts back to un-centered values
b_true = b_out - a_out*mean(xo)

```


```{r sol-plots}

#use best 2000 samples to estimate true value and stddev
idx = order(post_out, decreasing = TRUE)[1:2000]

a_sel = a_out[idx]
a_fit = median(a_sel)
a_err = sqrt(var(a_sel - a_fit))
a_sig = abs(a_fit - a)/a_err

b_sel = b_true[idx]
b_fit = median(b_sel)
b_err = sqrt(var(b_sel - b_fit))
b_sig = abs(b_fit - b)/b_err

#some diagnostic plots to see how we did
xvals = 1:Niter

plot(xvals, log10(post_out), type='p')
points(xvals[idx], log10(post_out[idx]), type='p', col='red', pch=1)

plot(1:Niter, a_out, type='p',pch=20, xlab='Iteration #')
abline(h=a, col='red', lwd=2.)
abline(h=a_fit, col='green', lwd=2.)
abline(h=c(a_fit-a_err, a_fit+a_err), col='green', lty='dashed')
legend(7000, 7.0, legend=c("True", 'Fit', '1sigma stddev'), col=c('red', 'green', 'green'), lty=c(1,1,2), lwd=2.)


plot(1:Niter, b_true, type='p',pch=20, xlab='Iteration #')
abline(h=b, col='red', lwd=2.)
abline(h=b_fit, col='green', lwd=2.)
abline(h=c(b_fit-b_err, b_fit+b_err), col='green', lty='dashed')
legend(7000, -20.0, legend=c("True", 'Fit', '1sigma stddev'), col=c('red', 'green', 'green'), lty=c(1,1,2), lwd=2.)


```



The Metropolis code seems to do a reasonable job of finding the correct answer, I purposefully set the input stddevs high to see if it could still converge on/near the correct value. I find:

```
a = `r round(a_fit, 3)` +/- `r round(a_err, 3)` (`r round(a_sig, 1)`sigma offset)
b = `r round(b_fit, 3)` +/- `r round(b_err, 3)` (`r round(b_sig, 1)`sigma offset)
```
although the uncertainties are far higher than they probably should be. Looking at the original data in the top plot, my intuition says that the uncertainties on `a` should be lower than those on `b` which is what I find above, but this is a very hand-wavy argument. I suspect there is some issue with my metropolis code, as better values of `b` (i.e., closer to 1) have slightly lower posterior values than the ones chosen as the "best fit" values. 

Also, my initial values are decently close to the input (true) values yet the metropolis code starts far from where it should (`a = 7, b = -30`) which I'm puzzled about. I'll take a look at the solutions when they're posted.












# Gibbs sampler  

Recall that with the Gibbs sampler there is no proposal distribution; we simply visit each parameter in sequence to update it. When we have updated each parameter---a complete cycle of such visits is called a _sweep_---the vector of updated parameters is our sample, and we save it. Rinse and repeat, as they say. Here is some detail to get you started:  

As in Metropolis above, suppose we are at position  
```
beta[j] = (x[j,1], x[j,2],..., x[j,N], a[j], b[j], lnsx[j], lnsy[j])
```  

in our $N+4$ dimensional parameter space.  

## Update x~1~  

The order in which we update the parameters should not matter. Here I update `x[1:N]` then `a`, then `b`, then `lnsy` then `lnsx`.  

To update $x_1$ we draw a sample from the function, $f(x)$ say, obtained from the posterior by discarding any factor that does not involve $x_1$.   

$$
f(x)={\rm dt}(y^{(o)}_1 \vert\, ax + b, e^{\ln\sigma_y}, \nu)
\cdot {\rm dnorm}(x^{(o)}_1 \vert\, x, e^{\ln\sigma_x}) 
\cdot {\rm dunif}(-10,20) \tag{10}
$$  
The t- and normal distributions are both symmetric, so we can swap their quantile and mean arguments (the first two arguments) to write  
$$
f(x)={\rm dt}(ax + b \vert\, y^{(o)}_1, e^{\ln\sigma_y}, \nu)
\cdot {\rm dnorm}(x \vert\, x^{(o)}_1, e^{\ln\sigma_x}) 
\cdot {\rm dunif}(-10,20) \tag{10}
$$
Here is a bit of code to sample from this function, written so that it can be used for any x-value, not just the first. It is untested, so it almost certainly has bugs.  

```{r newX}
newX <- function(id, xmin=-10, xmax=20, Nq=31) {
  ## arguments are...
  ## id, the data index. 
  ## xmin, minimum value of x[id]
  ## xmax, maximum value of x[id]
  ## Nq, number of points for the CDF
  
  ## from the calling environment we have
  ## j, the current sample number <------- NB
  ## nu, the normality
  ## xo[id], yo[id], the data
  ## current values a[j], b[j], lnsx[j], lnsy[j]

  q <- # q's for the piecwise linear CDF 
    seq(xmin, xmax, length.out=Nq) 

  d <- exp( # unscaled density
    dt.scaled(a[j]*q + b[j], df=nu, mean=yo[id], 
              sd=exp(lnsy[j]), log=TRUE) +
    dnorm(q, mean=xo[id], sd=exp(lnsx[j]), log=TRUE))
  
  ## make CDF p from density d
  p <- cumsum(diff(q)*0.5*(d[-1] + d[-Nq]))
  p <- c(0, p)
  p <- p/p[Nq] # piecewise linear CDF
  
  ## take one sample from the CDF
  u <- runif(1)
  ip1 <- sum(p<u) # position of largest  p < u
  ip2 <- ip1 + 1  # position of smallest p > u
  
  ## update x[id] in calling environment
  X[j+1, id] <<- (u - p[ip1])/(p[ip2] - p[ip1])*(q[ip2] - q[ip1])
  return(invisible(NULL))
}
```  

## Update a  

Only the y's depend on `a`, so we can discard the factors in the likelihood (9) that depend only on the x's. The result is 

$$
f(a) = \exp\sum_{i=1}^N  \ln{\rm dt}(y^{(o)}_i \vert\, ax_i + b, 
       e^{\ln\sigma_y}, \nu) \tag{10}
$$  

Here is a draft of a function to take a sample from $f(a)$. Notice the similarity to `newX()` defined above. Notice that we use the updated values of $x$.    

```{r newa}
newa <- function(amin=-10, amax=10, Nq=31) {
  ## arguments are...
  ## amin, minimum value of a
  ## amax, maximum value of a
  ## Nq, number of points for the CDF
  
  ## from the calling environment we have
  ## j, the current sample number <------- NB
  ## nu, the normality
  ## N, the number of x's
  ## yo[1:N], the data
  ## current values b[j], lnsx[j], lnsy[j]

  q <- # q's for the piecwise linear CDF 
    seq(amin, amax, length.out=Nq) 

  ## make piecewise linear density d
  sumlog <- double(N)  # recall N is number of data
  for (kd in 1:N) {    # sum over data
    sumlog <- sumlog + 
      dt.scaled(q*X[j+1,kd] + b[j], df=nu, mean=yo[kd], 
                sd=exp(lnsy[j]), log=TRUE)
  }
  d <- exp(sumlog) # piecwise linear density
    
  
  ## make CDF p from density d
  p <- cumsum(diff(q)*0.5*(d[-1] + d[-Nq]))
  p <- c(0, p)
  p <- p/p[Nq] # piecewise linear CDF
  
  ## take one sample from the CDF
  u <- runif(1)
  ip1 <- sum(p<u) # position of largest  p < u
  ip2 <- ip1 + 1  # position of smallest p > u
  
  ## update a in calling environment
  a[j+1] <<- (u - p[ip1])/(p[ip2] - p[ip1])*(q[ip2] - q[ip1])
  return(invisible(NULL))
}
```  

## Update b  

The function to update `b` is very similar to the one that updates `a`. Notice that we use the updated values of `x` and `a`.    

```{r newb}
newb <- function(bmin=-10, bmax=10, Nq=31) {
  ## arguments are...
  ## bmin, minimum value of b
  ## bmax, maximum value of b
  ## Nq, number of points for the CDF
  
  ## from the calling environment we have
  ## j, the current sample number <------- NB
  ## nu, the normality
  ## N, the number of x's
  ## yo[1:N], the y-data
  ## current values a[j], lnsx[j], lnsy[j]

  q <- # q's for the piecwise linear CDF 
    seq(bmin, bmax, length.out=Nq) 

  ## make piecewise linear density d
  sumlog <- double(N)  # recall N is number of data
  for (kd in 1:N) {    # sum over data
    sumlog <- sumlog + 
      dt.scaled(a[j+1]*X[j+1, kd] + q, df=nu, mean=yo[kd], 
                sd=exp(lnsy[j]), log=TRUE)
  }
  d <- exp(sumlog) # piecwise linear density
    
  
  ## make CDF p from density d
  p <- cumsum(diff(q)*0.5*(d[-1] + d[-Nq]))
  p <- c(0, p)
  p <- p/p[Nq] # piecewise linear CDF
  
  ## take one sample from the CDF
  u <- runif(1)
  ip1 <- sum(p<u) # position of largest  p < u
  ip2 <- ip1 + 1  # position of smallest p > u
  
  ## update b in calling environment
  b[j+1] <<- (u - p[ip1])/(p[ip2] - p[ip1])*(q[ip2] - q[ip1])
  return(invisible(NULL))
}
```  
## Update lnsy  

The function to update `lnsy` goes like this. Notice that we use the updated values of `x`, `a` and `b`.     

```{r, newlnsy}  
newlnsy <- function(lnsymin=-10, lnsymax=10, Nq=31) {
  ## arguments are...
  ## lnsymin, minimum value of lnsy
  ## lnsymax, maximum value of lnsy
  ## Nq, number of points for the CDF
  
  ## from the calling environment we have
  ## j, the current sample number <------- NB
  ## nu, the normality
  ## N, the number of x's
  ## yo[1:N], the y-data
  ## current values a[j], b[j] lnsx[j]

  q <- # q's for the piecwise linear CDF 
    seq(lnsymin, lnsymax, length.out=Nq) 

  ## make piecewise linear density d
  sumlog <- double(N)  # recall N is number of data
  for (kd in 1:N) {    # sum over data
    sumlog <- sumlog + 
              dt.scaled(a[j+1]*X[j+1, kd] + b[j+1], 
                        df=nu, mean=yo[kd], 
                        sd=exp(q), log=TRUE)
  }
  d <- exp(sumlog) # piecwise linear density
    
  
  ## make CDF p from density d
  p <- cumsum(diff(q)*0.5*(d[-1] + d[-Nq]))
  p <- c(0, p)
  p <- p/p[Nq] # piecewise linear CDF
  
  ## take one sample from the CDF
  u <- runif(1)
  ip1 <- sum(p<u) # position of largest  p < u
  ip2 <- ip1 + 1  # position of smallest p > u
  
  ## update lnsy in calling environment
  lnsy[j+1] <<- (u - p[ip1])/(p[ip2] - p[ip1])*(q[ip2] - q[ip1])
  return(invisible(NULL))
}
```  
## Update lnsx
The function to update `lnsx` goes like this. Notice that we use the updated values of `x`, `a`, `b` and `lnsy`.    

```{r newlnsx}  
newlnsx <- function(lnsxmin=-10, lnsxmax=10, Nq=31) {
  ## arguments are...
  ## lnsxmin, minimum value of lnsx
  ## lnsxmax, maximum value of lnsx
  ## Nq, number of points for the CDF
  
  ## from the calling environment we have
  ## j, the current sample number <------- NB
  ## nu, the normality
  ## N, the number of x's
  ## yo[1:N], the y-data
  ## current values a[j+1], b[j+1]

  q <- # q's for the piecwise linear CDF 
    seq(lnsxmin, lnsxmax, length.out=Nq) 

  ## make piecewise linear density d
  sumlog <- double(N)  # recall N is number of data
  for (kd in 1:N) {    # sum over data
    sumlog <- 
      sumlog + dnorm(X[j+1, kd], mean=xo[kd], sd=exp(q), log=TRUE)
  }
  d <- exp(sumlog) # piecwise linear density
    
  
  ## make CDF p from density d
  p <- cumsum(diff(q)*0.5*(d[-1] + d[-Nq]))
  p <- c(0, p)
  p <- p/p[Nq] # piecewise linear CDF
  
  ## take one sample from the CDF
  u <- runif(1)
  ip1 <- sum(p<u) # position of largest  p < u
  ip2 <- ip1 + 1  # position of smallest p > u
  
  ## update lnsy in calling environment
  lnsx[j+1] <<- (u - p[ip1])/(p[ip2] - p[ip1])*(q[ip2] - q[ip1])
  return(invisible(NULL))
}
```  

## Sampling  

We now put these functions to work in a Gibbs sampler.  

```{r sampling, eval=FALSE}
Ns <- 100 # number of samples, small while debugging
nu <- 3   # normality  

## preallocate storage
X <- matrix(rep(0, Ns*N), nrow=Ns)
a <- double(Ns)
b <- double(Ns)
lnsy <- double(Ns)
lnsx <- double(Ns)

## center x-values
xobar <- mean(xo)
xoo <- xo       # keep original x-data
xo <- xo - xbar # centered x-data

## initial values
X[1, ] <- xo
a[1] <- 1
b[1] <- 0
lnsx[1] <- 0
lnsy[1] <- 0

for (j in 1:(Ns-1)) {
  ## update the x's
  for (id in 1:N) {
    newX(id=id)
  }
  ## update the rest
  newa() 
  newb()
  newlnsy()
  newlnsx()
}
```

